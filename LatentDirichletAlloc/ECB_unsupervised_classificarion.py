# -*- coding: utf-8 -*-
"""UnsupervisedClassificarionECB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ziCYyO_EEUSYAfFjj_TbXzY5n6pQAr_J

# Unsupervised Learning ECB

# Data Loading
"""

# 1) We extract the data from a Reuters Euro news plus
import math
import tqdm
import pandas as pd
data = pd.read_csv('/content/Reuters_Euro_news_plus.csv', error_bad_lines=False);

print(data.head())

data_text = data[['body']]
data_text['id'] = data_text.index
documents = data_text

print("The doc length is: ",len(documents))
print(documents[:5])

"""# Data preprocessing"""

# 2) Data preprocessing

# 2.1) Tokenization: split the text into sentences and the sentences into words

# 2.2) Words with less than 3 letters are removed

# 2.3) All stopwords are removed

# 2.4) Words are lemmatized: words in third person are changed to first person and verbs in past and future in present

# 2.5) Words are stemmed: are reduced to their root form

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')

# we write a function for lemmatization and stemming (2.4) + 2.5))
stemmer = SnowballStemmer('english')
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

# we make a trial with one document to see the effect of lemmatization and stemming 
doc_sample = documents[documents['id'] == 4310].values[0][0]
print('original document: ')
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print('\n\n tokenized and lemmatized document: ')
print(preprocess(doc_sample))

# Now we can save the preprocessed articles in the variable 'processed_docs'
processed_docs = documents['body'].map(preprocess)
print (processed_docs[:10])

bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)  

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[processed_docs[0]]])

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

processed_docs = make_bigrams(processed_docs)

"""# Feature engineering: dictionaries and TFIDF"""

# 3) Now, starting from processed_docs, we create a dictionary in which it is written how many times a word appears in the training set.

dictionary = gensim.corpora.Dictionary(processed_docs)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break
# 3.1) Now, with the function 'filter_extremes' we can filter the tokens that appear in less than 15 documents(absolute value, out of 1 million documents in our dataset), or in more than 50% of the documents.
#Then after this step we keep only the first 100 000 most frequent tokens.
min_number = math.floor(len(documents)*15./1e6)
print ("min_number: ", min_number)
#keep_n=100000
dictionary.filter_extremes(no_below=min_number, no_above=0.5)

# 4) Now for each preprocessed document we create a dictionary. This dictionary contains 2D vectors related to the words in the prepocessed doc. Each vector has one index that identifies the word and the other that tells us how many times this word appear in the given document.
# We save the results in bow_corpus.

bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
print(bow_corpus[4310]) #In this way we select the doc 4310 

#We see explicitly the words of doc 4310 that are listed here

bow_doc_4310 = bow_corpus[4310]
for i in range(len(bow_doc_4310)):
    print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0],

dictionary[bow_doc_4310[i][0]],
bow_doc_4310[i][1]))

# 5) We pass from the term frequency(tf) representation to the TFIDF representation. 
#We apply the transformation to the bow_corpus (so the set in which there are the word frequency) and we save it to corpus_tfidf.
#To do this we first need to create an object tf-idf using 'models.TfidfModel' on bow_corpus.
#Then we apply this object to the entire corpus and we call the resuting corpu corpus_tfidf.
#Finally we preview the tfidf scores for our first doc
from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
from pprint import pprint
for doc in corpus_tfidf:
    pprint(doc)
    break

"""# LDA baseline"""

#6) Running LDA
#6.1) We train out LDA model using 'gensim.models.LdaMulticore'
work_num=2

lda_model = models.LdaMulticore(corpus=bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers = work_num)
 
#passes (optional) : number of passess through the entire corpus during training
#workers(optional) : set workers diretly to the number of your real cores

#6.2) For each topic we will explore the number of words occurring in that topic and its relative weight
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

print("This was tf:can you distinguish different topics using the words in each topic and their corresponding weights?")

# 6.3) Now we run the LDA using TF-IDF
lda_model_tfidf = models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=work_num)
for idx, topic in lda_model_tfidf.print_topics(-1):
    print('Topic: {} Word: {}'.format(idx, topic))
print("This was tfidf:can you distinguish different topics using the words in each topic and their corresponding weights?")

coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs,dictionary=dictionary, coherence='c_v')
coherence_lda_bow = coherence_model_lda.get_coherence()
print('Coherence Score bow: ', coherence_lda_bow)
# Compute Coherence Score Mallet
coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_docs,dictionary=dictionary, coherence='c_v')
coherence_lda_tfidf = coherence_model_lda.get_coherence()
print('Coherence Score tfidf: ', coherence_lda_tfidf)

!pip install pyLDAvis
import pyLDAvis.gensim
# Visualize the topics
LDAvis_prepared_bow = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)
pyLDAvis.display(LDAvis_prepared_bow)

LDAvis_prepared_tfidf = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary)
pyLDAvis.display(LDAvis_prepared_tfidf)

"""# LDA Mallet implementation"""

#!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
#!unzip mallet-2.0.8.zip
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models.wrappers import LdaMallet
from gensim import similarities
import os.path
os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'
mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this 
ldamallet10 = LdaMallet(mallet_path, corpus=bow_corpus, num_topics=10, id2word=dictionary)
# Show Topics
pprint(ldamallet10.show_topics(formatted=False))

# Compute Coherence Score without Mallet
coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs,dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)
# Compute Coherence Score Mallet
coherence_model_lda = CoherenceModel(model=ldamallet10, texts=processed_docs,dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

#coherence_model_ldamallet10 = CoherenceModel(model=ldamallet10, texts=bow_corpus, dictionary=dictionary, coherence='c_v')
#coherence_ldamallet10 = coherence_model_ldamallet10.get_coherence()
#print('\nCoherence Score: ', coherence_ldamallet10)

"""# A first look at the topics: improve interpretability by removing noise

Here we should look at terms present in a variety of topics because too general

# A first visualization: first problems and solutions
"""

#!pip install pyLDAvis
#import pyLDAvis.gensim
# Visualize the topics
LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)
pyLDAvis.display(LDAvis_prepared)

#Difference in visualization in tfidf
LDAvis_prepared_tfidf = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary)
pyLDAvis.display(LDAvis_prepared_tfidf)

"""# Hyperparameter tuning and performance assessment"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=num_topics, id2word=dictionary)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=processed_docs, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

# Can take a long time to run.
model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=processed_docs, start=4, limit=10, step=1)
# Show graph
limit=10; start=4; step=1;
x = range(start, limit, step)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()



# Print the coherence scores
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 4))

# Select the model and print the topics
optimal_model = model_list[5]
model_topics = optimal_model.show_topics(formatted=False)
pprint(optimal_model.print_topics(num_words=10))

"""# Training and visualization of the final optimal model

# Clustering with informative priors: a bayesian tale
"""

word_vector=['according','asset','authority','bank said','banking','biggest','billion euro','board',
'business','case','central bank','ceo','chief executive','company','cost','country','cut','deal','declined comment',
'equity','euro','european central','executive','finance','financial','fund','government','investigation',
'investment','investor','job','largest','last year','laundering','lender','loan','loss','make','management','market',
'may','merger','million euro','money laundering','need','new','one','operation','part','people','percent',
'plan','possible','potential','regulator','report','reported','risk','sale','scandal','share','shareholder','since',
'state','statement','supervisory','take','talk','top','transaction','unit','downgrade',
'profit warning','acquisition','outlook','capital increase','upgrade']
word_vector=" ".join(word_vector)
print(word_vector)

word_vector_short=['downgrade','profit warning','merger','acquisition','outlook','capital increase','upgrade',
                   'money laundering','bank','banking','cost','declined','ebitda','profit','loss','finance','risk','jobs','sales',
                  'debt','increase','decrease','worst','bad','good','report','investor']
word_vector_short=" ".join(word_vector_short)
print(word_vector_short)

!pip install guidedlda
import guidedlda

seed_topic_list = [
                   ['laundering', 'loss','cost', 'cut', 'downgrade','bad','decrease','warning', 'declined', 'worst'],
                   ['upgrade', 'gain', 'good','increase', 'profit'],
                   ['acquisition', 'merger', 'ebitda', 'jobs','sales', 'debt', 'report', 'investor' ]]

model = guidedlda.GuidedLDA(n_topics=5, n_iter=100, random_state=7, refresh=20)

seed_topics = {}
for t_id, st in enumerate(seed_topic_list):
    for word in st:
        seed_topics[dictionary[word]] = t_id

model.fit(processed_docs, seed_topics=seed_topics, seed_confidence=0.15)

n_top_words = 10
topic_word = model.topic_word_
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))

# 7) Performance evaluation for the tf model
#We do the performance evaluation on the document 4310
#We will get a score for each topic. This score corresponds to the probability thatour document's topic is that one (assigned from the LDA algorithm)

for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):

    print("\nScore for tf model: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 10)))

# 7.1) Performance evaluation for the tfidf model
#again we consider the document 4310

for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):
    print("\nScore for tfidf model: {}\t \nTopic: {}".format(score, lda_model_tfidf.print_topic(index, 10)))

# 8) Now we test our model on a new document (not inside the training test)
# We use the tf model
print ("Result from tf model")

unseen_document = 'How a Pentagon deal became an identity crisis for Google'
print(unseen_document)
bow_vector = dictionary.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
    print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))

# 8.1) Let us use the tfidf model
print ("Result from tfidf model /n")

for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):
    print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))

# 9) Coherence Score (modello base)

#Compute Model Perplexity and Coherence Score
#Let's calculate the baseline coherence score
from gensim.models import CoherenceModel

#9.1) Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_docs,dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)
